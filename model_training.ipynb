{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "## pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nilay\\Customer Churn Prediction\\venv\\lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4982 Training: 0.5039\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CustomerChurnModel:\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.preprocessor = None\n",
    "        LOG_FILE = f\"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log\" \n",
    "        logs_path = os.path.join(os.getcwd(), \"logs\", LOG_FILE)\n",
    "        os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "        LOG_FILE_PATH = os.path.join(logs_path, LOG_FILE)\n",
    "\n",
    "        logging.basicConfig(\n",
    "            filename=LOG_FILE_PATH,\n",
    "            format=\"[%(asctime)s] %(lineno)d %(name)s - %(levelname)s - %(message)s\",\n",
    "            level=logging.INFO\n",
    "        )\n",
    "\n",
    "    def data_ingestion(self):\n",
    "        try:\n",
    "            # divinding data in input and output features\n",
    "            data = pd.read_excel(self.data_path) \n",
    "            logging.info(\"Loading data from %s\", self.data_path)\n",
    "            X = data.drop(labels=['Churn', \"CustomerID\", \"Name\"], axis=1) #dropping unnecessary columns and assigning to X\n",
    "            Y = data[['Churn']] # selecting churn as Y\n",
    "            logging.info(\"Data_ingestion completed successfully\")\n",
    "            return X, Y\n",
    "        except Exception as e:\n",
    "            logging.info('Exception occured in data_ingestion', e)\n",
    "    \n",
    "    def cat_num(self, X,Y):\n",
    "        try:\n",
    "            # extracting categorical and numerical data \n",
    "            logging.info(\"Cat_num started\")\n",
    "            categorical_cols = X.select_dtypes(include='object').columns # object data will be added\n",
    "            numerical_cols = X.select_dtypes(exclude='object').columns # non object data will be added\n",
    "            logging.info(\"Cat_num completed successfully\")\n",
    "            return categorical_cols, numerical_cols\n",
    "        except Exception as e:\n",
    "            logging.info('Exception occured in cat_num', e)\n",
    "\n",
    "    def pipeline(self, categorical_cols, numerical_cols):\n",
    "        try:\n",
    "            logging.info(\"pipeline started\")\n",
    "            num_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='median')),# for imputing missing values using median as strategy\n",
    "                    ('scaler', StandardScaler())# scaling the numeric data\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            cat_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')), #imputing missing values using most frequent\n",
    "                    ('onehotencoder', OneHotEncoder(drop='first')) #encoding the categorical data\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            self.preprocessor = ColumnTransformer([\n",
    "                ('num_pipeline', num_pipeline, numerical_cols), #initializing the column transformer\n",
    "                ('cat_pipeline', cat_pipeline, categorical_cols)\n",
    "            ])\n",
    "            logging.info(\"pipeline completed successfully\")\n",
    "        except Exception as e:\n",
    "            logging.info('Exception occured in pipeline', e)\n",
    "\n",
    "    def train_test(self, X, Y):\n",
    "        try:\n",
    "            logging.info(\"train_test started\")\n",
    "            # splitting the data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=30)\n",
    "            logging.info(\"train_test completed successfully\")\n",
    "            return X_train, X_test, y_train, y_test\n",
    "        except Exception as e:\n",
    "            logging.info('Exception occured in train_test', e)\n",
    "\n",
    "    def feature_scaling(self, X_train, X_test):\n",
    "        try:\n",
    "            # using pipeline and fit_transforming the train and transforming the test data\n",
    "            logging.info(\"feature_scaling started\")\n",
    "            X_train_scaled = pd.DataFrame(self.preprocessor.fit_transform(X_train), columns=self.preprocessor.get_feature_names_out())\n",
    "            X_test_scaled = pd.DataFrame(self.preprocessor.transform(X_test), columns=self.preprocessor.get_feature_names_out())\n",
    "            logging.info(\"feature_scaling completed successfully\")\n",
    "            return X_train_scaled, X_test_scaled\n",
    "        except Exception as e:\n",
    "            logging.info('Exception occured in feature_scaling', e)\n",
    "\n",
    "    def model_train(self, X_train, X_test, y_train, y_test):\n",
    "        try:\n",
    "            logging.info(\"model_train started\")\n",
    "            classifier = RandomForestClassifier(n_estimators=1000,   # number of trees in forest\n",
    "                                                criterion='gini',    # criterion method\n",
    "                                                ccp_alpha=0.3,       # cost complexity pruning\n",
    "                                                max_depth=100,       # maximum depth\n",
    "                                                min_samples_split=5, # minimum number of samples\n",
    "                                                min_samples_leaf=2,  # number of leaf \n",
    "                                                max_features=\"sqrt\", # maximum features\n",
    "                                                random_state=42)        \n",
    "\n",
    "            # Fitting the model on the training data\n",
    "            classifier.fit(X_train, y_train)\n",
    "\n",
    "            logging.info(\"model.fit completed successfully\")\n",
    "\n",
    "            # Making predictions on the testing data\n",
    "            y_pred = classifier.predict(X_test)\n",
    "\n",
    "            logging.info(\"model prediction completed\")\n",
    "\n",
    "            # Evaluating the model's performance and training accuracy\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "            logging.info(\"accuracy score: %s\", accuracy)\n",
    "\n",
    "            # Checking training score\n",
    "            train_score = classifier.score(X_train, y_train)\n",
    "            logging.info(\"train score: %s\", train_score)\n",
    "            print(f\"Accuracy: {accuracy:.4f} Training: {train_score:.4f}\")\n",
    "        except Exception as e:\n",
    "            logging.info('Exception occured in data_ingestion', e)\n",
    "\n",
    "data_path = \"data/customer_churn_large_dataset.xlsx\"\n",
    "churn_model = CustomerChurnModel(data_path)\n",
    "X, Y = churn_model.data_ingestion()\n",
    "categorical_cols, numerical_cols = churn_model.cat_num(X, Y)\n",
    "churn_model.pipeline(categorical_cols, numerical_cols)\n",
    "X_train, X_test, y_train, y_test = churn_model.train_test(X, Y)\n",
    "X_train_scaled, X_test_scaled = churn_model.feature_scaling(X_train, X_test)\n",
    "churn_model.model_train(X_train_scaled, X_test_scaled, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
